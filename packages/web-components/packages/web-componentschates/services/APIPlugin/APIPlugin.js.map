{"version":3,"sources":["services/APIPlugin/APIPlugin.ts","services/APIPlugin/APIPlugin.js"],"names":["_bam_key","import","meta","env","VITE_BAM_KEY","_watsonxai_key","VITE_WATSONXAI_KEY","_watsonxai_project_id","VITE_WATSONXAI_PROJECT_ID","APIPlugin","sendMessageWatsonX","API_URL","model","temperature","userPrompt","messages","message","session","eventNumber","console","log","failed","reply","max_tokens","user_name","agent_name","initial_prompt","model_id","parameters","decoding_method","max_new_tokens","min_new_tokens","stop_sequences","repetition_penalty","project_id","history","map","conv","origin","text","join","watsonxPayload","b64_encoded_inputs","hapEnabled","hapText","locale","projectId","promptData","imput","modelId","modelParameters","requestOptions","method","headers","Authorization","body","JSON","stringify","fetch","then","response","json","error","sendMessageBAM","completePrompt","top_k","top_p","stream","bam_payload","inputs","sendMessageLocal","prompt","type","reminder","context","entry","frequency_penalty","presence_penalty","n","payload","user_id","event","max_tries","split"],"mappings":"AAAA;ACCA;AACA;AACA;AACA;AACA;AACA;AACA;ADEA;ACAA;AACA;AACA;ADEA,MAAMA,QAAQ;AACZ;AACAC,MAAM,CAACC,IAAI,CAACC,GAAG,IAAIF,MAAM,CAACC,IAAI,CAACC,GAAG,CAACC,YAAY;AAEjD,MAAMC,cAAc;AAClB;AACAJ,MAAM,CAACC,IAAI,CAACC,GAAG,IAAIF,MAAM,CAACC,IAAI,CAACC,GAAG,CAACG,kBAAkB;AAEvD,MAAMC,qBAAqB;AACzB;AACAN,MAAM,CAACC,IAAI,CAACC,GAAG,IAAIF,MAAM,CAACC,IAAI,CAACC,GAAG,CAACK,yBAAyB;AAE9D;ACHA;AACA;ADKA,MAAMC,SAAS,CAAA;EACb;ACHF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EDKE,aAAaC,kBAAkBA,CAC7BC,OAAO,EACPC,KAAK,EACLC,WAAW,EACXC,UAAU,EACVC,QAAQ,EACRC,OAAO,EACPC,OAAO,EACPC,WAAW,EAAA;IAEXC,OAAO,CAACC,GAAG,CAAC,uBAAuB,CAAC;IACpCD,OAAO,CAACC,GAAG,CAACN,UAAU,CAAC;IACvBK,OAAO,CAACC,GAAG,CAACP,WAAW,CAAC;IACxBM,OAAO,CAACC,GAAG,CAACJ,OAAO,CAAC;IACpBG,OAAO,CAACC,GAAG,CAACF,WAAW,CAAC;IACxBC,OAAO,CAACC,GAAG,CAACH,OAAO,CAAC;IACpB,IAAI,CAACZ,cAAc,EAAE;MACnB,OAAO;QACLgB,MAAM,EAAE,IAAI;QACZC,KAAK,EACH;MCbE,CDcL;IACH;IACA,MAAMC,UAAU,GAAG,KAAK;IACxB,MAAMC,SAAS,GAAG,MAAM;IACxB,MAAMC,UAAU,GAAG,KAAK;IACxB,MAAMC,cAAc,GAAG;ACb3B,KDcK;IACD,IAAIC,QAAQ,GAAG,6BAA6B;IAC5C,IAAIf,KAAK,IAAI,SAAS,EAAE;MACtBe,QAAQ,GAAG,SAAS;IACtB;IAEA,MAAMC,UAAU,GAAG;MACjBC,eAAe,EAAE,QAAQ;MACzBC,cAAc,EAAEP,UAAU;MAC1BQ,cAAc,EAAE,CAAC;MACjBC,cAAc,EAAE,CAACR,SAAS,EAAEC,UAAU,CAAC;MACvCQ,kBAAkB,EAAE;ICdlB,CDeH;IAED,MAAMC,UAAU,GAAG3B,qBAAqB;IACxC,IAAI2B,UAAU,IAAI,IAAI,EAAE;MACtB,OAAO;QACLb,MAAM,EAAE,IAAI;QACZC,KAAK,EACH;MChBE,CDiBL;IACH;IAEA,MAAMa,OAAO,GAAGpB,QAAQ,CACrBqB,GAAG,CAAEC,IAAI,IAAI;MACZ,OAAOA,IAAI,CAACC,MAAM,GAAG,GAAG,GAAGD,IAAI,CAACE,IAAI;IACtC,CAAC,CAAC,CACDC,IAAI,CAAC,IAAI,CAAC;IAEb,MAAMC,cAAc,GAAG;MACrBC,kBAAkB,EAAE,IAAI;MACxBC,UAAU,EAAE,IAAI;MAChBC,OAAO,EAAElB,cAAc;MACvBmB,MAAM,EAAE,IAAI;MACZC,SAAS,EAAEZ,UAAU;MACrBa,UAAU,EAAE;QACVC,KAAK,EAAEb,OAAO;QACdc,OAAO,EAAEtB,QAAQ;QACjBuB,eAAe,EAAEtB;MClBb;IACJ,CDmBH;IAEDT,OAAO,CAACC,GAAG,CAACqB,cAAc,CAAC;IAC3BtB,OAAO,CAACC,GAAG,CAACf,cAAc,CAAC;IAE3B,MAAM8C,cAAc,GAAG;MACrBC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClCC,aAAa,EAAE,SAAS,GAAGjD;MCpBvB,CDqBL;MACDkD,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAChB,cAAc;ICpBjC,CDqBH;IAED,IAAI;MACF,OAAO,MAAMiB,KAAK,CAAC/C,OAAO,EAAEwC,cAAc,CAAC,CACxCQ,IAAI,CAAEC,QAAQ,IAAKA,QAAQ,CAACC,IAAI,CAAA,CAAE,CAAC,CACnCF,IAAI,CAAEC,QAAQ,IAAKA,QAAQ,CAAC;IACjC,CAAC,CAAC,OAAOE,KAAU,EAAE;MACnB,OAAO;QACLxC,KAAK,EAAE,kBAAkB,GAAGX,OAAO,GAAG,YAAY,GAAGmD,KAAK,CAAC9C,OAAO;QAClEK,MAAM,EAAE;MCpBJ,CDqBL;IACH;EACF;EAEA;ACrBF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EDuBE,aAAa0C,cAAcA,CACzBpD,OAAO,EACPC,KAAK,EACLC,WAAW,EACXC,UAAU,EACVC,QAAQ,EACRC,OAAO,EACPC,OAAO,EACPC,WAAW,EAAA;IAEXC,OAAO,CAACC,GAAG,CAAC,mBAAmB,CAAC;IAChCD,OAAO,CAACC,GAAG,CAACN,UAAU,CAAC;IACvBK,OAAO,CAACC,GAAG,CAACJ,OAAO,CAAC;IACpBG,OAAO,CAACC,GAAG,CAACF,WAAW,CAAC;IACxBC,OAAO,CAACC,GAAG,CAACH,OAAO,CAAC;IAEpB,IAAI,CAACjB,QAAQ,EAAE;MACb,OAAO;QACLqB,MAAM,EAAE,IAAI;QACZC,KAAK,EACH;MChCE,CDiCL;IACH;IACA,MAAME,SAAS,GAAG,MAAM;IACxB,MAAMC,UAAU,GAAG,KAAK;IACxB,MAAMC,cAAc,GAAG,iFAAiF;IAExG,MAAMS,OAAO,GAAGpB,QAAQ,CACrBqB,GAAG,CAAEC,IAAI,IAAI;MACZ,OAAOA,IAAI,CAACC,MAAM,GAAG,GAAG,GAAGD,IAAI,CAACE,IAAI;IACtC,CAAC,CAAC,CACDC,IAAI,CAAC,IAAI,CAAC;IAEb,MAAMb,QAAQ,GAAG,aAAa,GAAGf,KAAK,GAAG,WAAW;IAEpD,MAAMoD,cAAc,GAAGtC,cAAc,GAAG,IAAI,GAAGS,OAAO;IAEtD,MAAM8B,KAAK,GAAG,EAAE;IAChB,MAAMC,KAAK,GAAG,GAAG;IAEjB,MAAMtC,UAAU,GAAG;MACjBC,eAAe,EAAE,QAAQ;MACzBsC,MAAM,EAAE,KAAK;MACbtD,WAAW,EAAEA,WAAW;MACxBoD,KAAK,EAAEA,KAAK;MACZC,KAAK,EAAEA,KAAK;MACZpC,cAAc,EAAE,IAAI;MACpBE,cAAc,EAAE,CAACR,SAAS,EAAEC,UAAU;ICrCpC,CDsCH;IAED,MAAM2C,WAAW,GAAG;MAClBzC,QAAQ,EAAEA,QAAQ;MAClB0C,MAAM,EAAE,CAACL,cAAc,CAAC;MACxBpC,UAAU,EAAEA;ICtCV,CDuCH;IACD,MAAMuB,cAAc,GAAG;MACrBC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClCC,aAAa,EAAE,SAAS,GAAGtD;MCtCvB,CDuCL;MACDuD,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACW,WAAW;ICtC9B,CDuCH;IAED,IAAI;MACF,OAAO,MAAMV,KAAK,CAAC/C,OAAO,EAAEwC,cAAc,CAAC,CACxCQ,IAAI,CAAEC,QAAQ,IAAKA,QAAQ,CAACC,IAAI,CAAA,CAAE,CAAC,CACnCF,IAAI,CAAEC,QAAQ,IAAKA,QAAQ,CAAC;IACjC,CAAC,CAAC,OAAOE,KAAU,EAAE;MACnB,OAAO;QACLxC,KAAK,EAAE,kBAAkB,GAAGX,OAAO,GAAG,YAAY,GAAGmD,KAAK,CAAC9C,OAAO;QAClEK,MAAM,EAAE;MCtCJ,CDuCL;IACH;EACF;EAEA;ACvCF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EDyCE,aAAaiD,gBAAgBA,CAC3B3D,OAAO,EACPC,KAAK,EACLC,WAAW,EACXC,UAAU,EACVC,QAAQ,EACRC,OAAO,EACPC,OAAO,EACPC,WAAW,EAAA;IAEXC,OAAO,CAACC,GAAG,CACT,WAAW,GACTT,OAAO,GACP,cAAc,GACdC,KAAK,GACL,UAAU,GACVC,WAAW,GACX,GAAG,CACN;IACD,MAAMU,UAAU,GAAG,IAAI;IACvB,MAAMC,SAAS,GAAG,MAAM;IACxB,MAAMC,UAAU,GAAG,KAAK;IACxB,IAAIC,cAAc,GAChB,iBAAiB,GACjBZ,UAAU,GACV,8EAA8E;IAEhFY,cAAc,GAAG,iBAAiB,GAAGZ,UAAU,GAAG,mBAAmB;IAErE,IAAIyD,MAAM,GACR7C,cAAc,GACdX,QAAQ,CACLqB,GAAG,CAAEpB,OAAO,IAAI;MACf,OAAOA,OAAO,CAACwD,IAAI,IAAI,KAAK,GACxB,QAAQ,GAAGxD,OAAO,CAACuB,IAAI,GAAG,SAAS,GACnCvB,OAAO,CAACuB,IAAI;IAClB,CAAC,CAAC,CACDC,IAAI,CAAC,IAAI,CAAC;IAEf,MAAML,OAAO,GAAGpB,QAAQ,CACrBqB,GAAG,CAAEC,IAAI,IAAI;MACZ,OAAOA,IAAI,CAACmC,IAAI,GAAG,GAAG,GAAGnC,IAAI,CAACE,IAAI;IACpC,CAAC,CAAC,CACDC,IAAI,CAAC,IAAI,CAAC;IAEb+B,MAAM,GAAG7C,cAAc;IAEvB,MAAM+C,QAAQ,GAAG,EAAE;IACnB,MAAMC,OAAO,GAAGvC,OAAO;IACvB,MAAMwC,KAAK,GAAG3D,OAAO;IACrB,MAAMkD,KAAK,GAAG,GAAG;IACjB,MAAMU,iBAAiB,GAAG,GAAG;IAC7B,MAAMC,gBAAgB,GAAG,GAAG;IAC5B,MAAMC,CAAC,GAAG,CAAC;IAEX,MAAMC,OAAO,GAAG;MACdC,OAAO,EAAE,QAAQ;MACjB/D,OAAO,EAAEA,OAAO;MAChBgE,KAAK,EAAE/D,WAAW;MAClBqD,MAAM,EAAEA,MAAM;MACdE,QAAQ,EAAEA,QAAQ;MAClBC,OAAO,EAAEA,OAAO;MAChBC,KAAK,EAAEA,KAAK;MACZ9D,WAAW,EAAEA,WAAW;MACxBU,UAAU,EAAEA,UAAU;MACtB2C,KAAK,EAAEA,KAAK;MACZU,iBAAiB,EAAEA,iBAAiB;MACpCC,gBAAgB,EAAEA,gBAAgB;MAClCC,CAAC,EAAEA,CAAC;MACJtD,SAAS,EAAEA,SAAS;MACpBC,UAAU,EAAEA,UAAU;MACtByD,SAAS,EAAE;IC1DT,CD2DH;IAED/D,OAAO,CAACC,GAAG,CAAC2D,OAAO,CAAC;IAEpB,MAAM5B,cAAc,GAAG;MACrBC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QAAE,cAAc,EAAE;MAAkB,CAAE;MAC/CE,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACsB,OAAO;IC5D1B,CD6DH;IAED,IAAI;MACF,OAAO,MAAMrB,KAAK,CAAC/C,OAAO,EAAEwC,cAAc,CAAC,CACxCQ,IAAI,CAAEC,QAAQ,IAAKA,QAAQ,CAACC,IAAI,CAAA,CAAE,CAAC,CACnCF,IAAI,CAAEC,QAAQ,IAAI;QACjBzC,OAAO,CAACC,GAAG,CAACwC,QAAQ,CAACtC,KAAK,CAAC;QAC3B,IAAIV,KAAK,IAAI,SAAS,EAAE;UACtBgD,QAAQ,CAACtC,KAAK,GAAGsC,QAAQ,CAACtC,KAAK,CAAC6D,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;QACvD;QACA,OAAOvB,QAAQ;MACjB,CAAC,CAAC;IACN,CAAC,CAAC,OAAOE,KAAU,EAAE;MACnB,OAAO;QACLxC,KAAK,EAAE,kBAAkB,GAAGX,OAAO,GAAG,YAAY,GAAGmD,KAAK,CAAC9C,OAAO;QAClEK,MAAM,EAAE;MC5DJ,CD6DL;IACH;EACF;AC5DF;AD8DA,eAAeZ,SAAS","file":"APIPlugin.js","sourcesContent":["/**\n * @license\n *\n * Copyright IBM Corp. 2023\n *\n * This source code is licensed under the Apache-2.0 license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n/**\n * @constant {string | string} key for the API calls\n * @private\n */\nconst _bam_key =\n  //@ts-ignore\n  import.meta.env && import.meta.env.VITE_BAM_KEY;\n\nconst _watsonxai_key =\n  //@ts-ignore\n  import.meta.env && import.meta.env.VITE_WATSONXAI_KEY;\n\nconst _watsonxai_project_id =\n  //@ts-ignore\n  import.meta.env && import.meta.env.VITE_WATSONXAI_PROJECT_ID;\n\n/**\n * LlamaPlugin piece to fetch data from user defined api url\n */\nclass APIPlugin {\n  /**\n   * Gets WatsonX payload\n   * @param {string} API_URL - user defined url for query\n   * @param {string} model - selected model within the API\n   * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n   * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n   * @param {Object[]} messages - previous message history array for context\n   * @param {string} message - current message sent by the user\n   * @param {string} session -  unique ID to differentiate calls within the API\n   * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n   * @returns {Promise<any>} Response data from the api\n   * @example\n   * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n   */\n  static async sendMessageWatsonX(\n    API_URL,\n    model,\n    temperature,\n    userPrompt,\n    messages,\n    message,\n    session,\n    eventNumber\n  ) {\n    console.log('sending to WatsonX...');\n    console.log(userPrompt);\n    console.log(temperature);\n    console.log(message);\n    console.log(eventNumber);\n    console.log(session);\n    if (!_watsonxai_key) {\n      return {\n        failed: true,\n        reply:\n          'Error: No Watsonx-ai API key specified, please append your key to your .env root file in order to access the Watson service',\n      };\n    }\n    const max_tokens = 10000;\n    const user_name = 'user';\n    const agent_name = 'bot';\n    const initial_prompt = `[INST] <<SYS>> You are Watson, you'll answer all my questions. <</SYS>> [/INST]\n    `;\n    let model_id = 'meta-llama/llama-2-70b-chat';\n    if (model == 'granite') {\n      model_id = 'granite';\n    }\n\n    const parameters = {\n      decoding_method: 'greedy',\n      max_new_tokens: max_tokens,\n      min_new_tokens: 0,\n      stop_sequences: [user_name, agent_name],\n      repetition_penalty: 1,\n    };\n\n    const project_id = _watsonxai_project_id;\n    if (project_id == null) {\n      return {\n        failed: true,\n        reply:\n          'No Watsonx-ai project id specified, please add it to your .env file',\n      };\n    }\n\n    const history = messages\n      .map((conv) => {\n        return conv.origin + ':' + conv.text;\n      })\n      .join('\\n');\n\n    const watsonxPayload = {\n      b64_encoded_inputs: true,\n      hapEnabled: true,\n      hapText: initial_prompt,\n      locale: 'en',\n      projectId: project_id,\n      promptData: {\n        imput: history,\n        modelId: model_id,\n        modelParameters: parameters,\n      },\n    };\n\n    console.log(watsonxPayload);\n    console.log(_watsonxai_key);\n\n    const requestOptions = {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        Authorization: 'Bearer ' + _watsonxai_key,\n      },\n      body: JSON.stringify(watsonxPayload),\n    };\n\n    try {\n      return await fetch(API_URL, requestOptions)\n        .then((response) => response.json())\n        .then((response) => response);\n    } catch (error: any) {\n      return {\n        reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n        failed: true,\n      };\n    }\n  }\n\n  /**\n   * Gets BAM internal research resource payload\n   * @param {string} API_URL - user defined url for query\n   * @param {string} model - selected model within the API\n   * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n   * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n   * @param {Object[]} messages - previous message history array for context\n   * @param {string} message - current message sent by the user\n   * @param {string} session -  unique ID to differentiate calls within the API\n   * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n   * @returns {Promise<any>} Response data from the api\n   * @example\n   * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n   */\n  static async sendMessageBAM(\n    API_URL,\n    model,\n    temperature,\n    userPrompt,\n    messages,\n    message,\n    session,\n    eventNumber\n  ) {\n    console.log('sending to BAM...');\n    console.log(userPrompt);\n    console.log(message);\n    console.log(eventNumber);\n    console.log(session);\n\n    if (!_bam_key) {\n      return {\n        failed: true,\n        reply:\n          'Error: No API key specified, please append your key to your .env root file or Vite file in order to access the BAM service',\n      };\n    }\n    const user_name = 'user';\n    const agent_name = 'bot';\n    const initial_prompt = `[INST] <<SYS>> You are Watson, you'll answer all my questions. <</SYS>> [/INST]`;\n\n    const history = messages\n      .map((conv) => {\n        return conv.origin + ':' + conv.text;\n      })\n      .join('\\n');\n\n    const model_id = 'meta-llama/' + model + '-70b-chat';\n\n    const completePrompt = initial_prompt + '\\n' + history;\n\n    const top_k = 50;\n    const top_p = 1.0;\n\n    const parameters = {\n      decoding_method: 'sample',\n      stream: false,\n      temperature: temperature,\n      top_k: top_k,\n      top_p: top_p,\n      max_new_tokens: 1024,\n      stop_sequences: [user_name, agent_name],\n    };\n\n    const bam_payload = {\n      model_id: model_id,\n      inputs: [completePrompt],\n      parameters: parameters,\n    };\n    const requestOptions = {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        Authorization: 'Bearer ' + _bam_key,\n      },\n      body: JSON.stringify(bam_payload),\n    };\n\n    try {\n      return await fetch(API_URL, requestOptions)\n        .then((response) => response.json())\n        .then((response) => response);\n    } catch (error: any) {\n      return {\n        reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n        failed: true,\n      };\n    }\n  }\n\n  /**\n   * Gets Static url payload\n   * @param {string} API_URL - user defined url for query\n   * @param {string} model - selected model within the API\n   * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n   * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n   * @param {Object[]} messages - previous message history array for context\n   * @param {string} message - current message sent by the user\n   * @param {string} session -  unique ID to differentiate calls within the API\n   * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n   * @returns {Promise<any>} Response data from the api\n   * @example\n   * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n   */\n  static async sendMessageLocal(\n    API_URL,\n    model,\n    temperature,\n    userPrompt,\n    messages,\n    message,\n    session,\n    eventNumber\n  ) {\n    console.log(\n      'querying ' +\n        API_URL +\n        ' with model:' +\n        model +\n        ' (temp: ' +\n        temperature +\n        ')'\n    );\n    const max_tokens = 1000;\n    const user_name = 'user';\n    const agent_name = 'bot';\n    let initial_prompt =\n      '[INST] <<SYS>> ' +\n      userPrompt +\n      ' If returning code of any kind you must use \"```\" delimiters<</SYS>> [/INST]';\n\n    initial_prompt = '[INST] <<SYS>> ' + userPrompt + ' <</SYS>> [/INST]';\n\n    let prompt =\n      initial_prompt +\n      messages\n        .map((message) => {\n          return message.type == 'bot'\n            ? '[INST]' + message.text + '[/INST]'\n            : message.text;\n        })\n        .join('\\n');\n\n    const history = messages\n      .map((conv) => {\n        return conv.type + ':' + conv.text;\n      })\n      .join('\\n');\n\n    prompt = initial_prompt;\n\n    const reminder = '';\n    const context = history;\n    const entry = message;\n    const top_p = 0.0;\n    const frequency_penalty = 1.0;\n    const presence_penalty = 0.0;\n    const n = 1;\n\n    const payload = {\n      user_id: 'xxxxxx',\n      session: session,\n      event: eventNumber,\n      prompt: prompt,\n      reminder: reminder,\n      context: context,\n      entry: entry,\n      temperature: temperature,\n      max_tokens: max_tokens,\n      top_p: top_p,\n      frequency_penalty: frequency_penalty,\n      presence_penalty: presence_penalty,\n      n: n,\n      user_name: user_name,\n      agent_name: agent_name,\n      max_tries: 3,\n    };\n\n    console.log(payload);\n\n    const requestOptions = {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(payload),\n    };\n\n    try {\n      return await fetch(API_URL, requestOptions)\n        .then((response) => response.json())\n        .then((response) => {\n          console.log(response.reply);\n          if (model == 'llama-2') {\n            response.reply = response.reply.split('undefined')[0];\n          }\n          return response;\n        });\n    } catch (error: any) {\n      return {\n        reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n        failed: true,\n      };\n    }\n  }\n}\nexport default APIPlugin;\n","/**\n * @license\n *\n * Copyright IBM Corp. 2023\n *\n * This source code is licensed under the Apache-2.0 license found in the\n * LICENSE file in the root directory of this source tree.\n */\n/**\n * @constant {string | string} key for the API calls\n * @private\n */\nconst _bam_key = \n//@ts-ignore\nimport.meta.env && import.meta.env.VITE_BAM_KEY;\nconst _watsonxai_key = \n//@ts-ignore\nimport.meta.env && import.meta.env.VITE_WATSONXAI_KEY;\nconst _watsonxai_project_id = \n//@ts-ignore\nimport.meta.env && import.meta.env.VITE_WATSONXAI_PROJECT_ID;\n/**\n * LlamaPlugin piece to fetch data from user defined api url\n */\nclass APIPlugin {\n    /**\n     * Gets WatsonX payload\n     * @param {string} API_URL - user defined url for query\n     * @param {string} model - selected model within the API\n     * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n     * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n     * @param {Object[]} messages - previous message history array for context\n     * @param {string} message - current message sent by the user\n     * @param {string} session -  unique ID to differentiate calls within the API\n     * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n     * @returns {Promise<any>} Response data from the api\n     * @example\n     * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n     */\n    static async sendMessageWatsonX(API_URL, model, temperature, userPrompt, messages, message, session, eventNumber) {\n        console.log('sending to WatsonX...');\n        console.log(userPrompt);\n        console.log(temperature);\n        console.log(message);\n        console.log(eventNumber);\n        console.log(session);\n        if (!_watsonxai_key) {\n            return {\n                failed: true,\n                reply: 'Error: No Watsonx-ai API key specified, please append your key to your .env root file in order to access the Watson service',\n            };\n        }\n        const max_tokens = 10000;\n        const user_name = 'user';\n        const agent_name = 'bot';\n        const initial_prompt = `[INST] <<SYS>> You are Watson, you'll answer all my questions. <</SYS>> [/INST]\n    `;\n        let model_id = 'meta-llama/llama-2-70b-chat';\n        if (model == 'granite') {\n            model_id = 'granite';\n        }\n        const parameters = {\n            decoding_method: 'greedy',\n            max_new_tokens: max_tokens,\n            min_new_tokens: 0,\n            stop_sequences: [user_name, agent_name],\n            repetition_penalty: 1,\n        };\n        const project_id = _watsonxai_project_id;\n        if (project_id == null) {\n            return {\n                failed: true,\n                reply: 'No Watsonx-ai project id specified, please add it to your .env file',\n            };\n        }\n        const history = messages\n            .map((conv) => {\n            return conv.origin + ':' + conv.text;\n        })\n            .join('\\n');\n        const watsonxPayload = {\n            b64_encoded_inputs: true,\n            hapEnabled: true,\n            hapText: initial_prompt,\n            locale: 'en',\n            projectId: project_id,\n            promptData: {\n                imput: history,\n                modelId: model_id,\n                modelParameters: parameters,\n            },\n        };\n        console.log(watsonxPayload);\n        console.log(_watsonxai_key);\n        const requestOptions = {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                Authorization: 'Bearer ' + _watsonxai_key,\n            },\n            body: JSON.stringify(watsonxPayload),\n        };\n        try {\n            return await fetch(API_URL, requestOptions)\n                .then((response) => response.json())\n                .then((response) => response);\n        }\n        catch (error) {\n            return {\n                reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n                failed: true,\n            };\n        }\n    }\n    /**\n     * Gets BAM internal research resource payload\n     * @param {string} API_URL - user defined url for query\n     * @param {string} model - selected model within the API\n     * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n     * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n     * @param {Object[]} messages - previous message history array for context\n     * @param {string} message - current message sent by the user\n     * @param {string} session -  unique ID to differentiate calls within the API\n     * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n     * @returns {Promise<any>} Response data from the api\n     * @example\n     * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n     */\n    static async sendMessageBAM(API_URL, model, temperature, userPrompt, messages, message, session, eventNumber) {\n        console.log('sending to BAM...');\n        console.log(userPrompt);\n        console.log(message);\n        console.log(eventNumber);\n        console.log(session);\n        if (!_bam_key) {\n            return {\n                failed: true,\n                reply: 'Error: No API key specified, please append your key to your .env root file or Vite file in order to access the BAM service',\n            };\n        }\n        const user_name = 'user';\n        const agent_name = 'bot';\n        const initial_prompt = `[INST] <<SYS>> You are Watson, you'll answer all my questions. <</SYS>> [/INST]`;\n        const history = messages\n            .map((conv) => {\n            return conv.origin + ':' + conv.text;\n        })\n            .join('\\n');\n        const model_id = 'meta-llama/' + model + '-70b-chat';\n        const completePrompt = initial_prompt + '\\n' + history;\n        const top_k = 50;\n        const top_p = 1.0;\n        const parameters = {\n            decoding_method: 'sample',\n            stream: false,\n            temperature: temperature,\n            top_k: top_k,\n            top_p: top_p,\n            max_new_tokens: 1024,\n            stop_sequences: [user_name, agent_name],\n        };\n        const bam_payload = {\n            model_id: model_id,\n            inputs: [completePrompt],\n            parameters: parameters,\n        };\n        const requestOptions = {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                Authorization: 'Bearer ' + _bam_key,\n            },\n            body: JSON.stringify(bam_payload),\n        };\n        try {\n            return await fetch(API_URL, requestOptions)\n                .then((response) => response.json())\n                .then((response) => response);\n        }\n        catch (error) {\n            return {\n                reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n                failed: true,\n            };\n        }\n    }\n    /**\n     * Gets Static url payload\n     * @param {string} API_URL - user defined url for query\n     * @param {string} model - selected model within the API\n     * @param {string} temperature - floating number ranging from 0 to 1, dictates how creative the response will be,\n     * @param {string} userPrompt - additional behvaior prompt appended to the system prompt, given by the user as a component parameter\n     * @param {Object[]} messages - previous message history array for context\n     * @param {string} message - current message sent by the user\n     * @param {string} session -  unique ID to differentiate calls within the API\n     * @param {string} eventNumber - message count provided as an inner parameter of the chat component\n     * @returns {Promise<any>} Response data from the api\n     * @example\n     * import { LlamaPluginAPI } from '@carbon/ibmdotcom-services';\n     */\n    static async sendMessageLocal(API_URL, model, temperature, userPrompt, messages, message, session, eventNumber) {\n        console.log('querying ' +\n            API_URL +\n            ' with model:' +\n            model +\n            ' (temp: ' +\n            temperature +\n            ')');\n        const max_tokens = 1000;\n        const user_name = 'user';\n        const agent_name = 'bot';\n        let initial_prompt = '[INST] <<SYS>> ' +\n            userPrompt +\n            ' If returning code of any kind you must use \"```\" delimiters<</SYS>> [/INST]';\n        initial_prompt = '[INST] <<SYS>> ' + userPrompt + ' <</SYS>> [/INST]';\n        let prompt = initial_prompt +\n            messages\n                .map((message) => {\n                return message.type == 'bot'\n                    ? '[INST]' + message.text + '[/INST]'\n                    : message.text;\n            })\n                .join('\\n');\n        const history = messages\n            .map((conv) => {\n            return conv.type + ':' + conv.text;\n        })\n            .join('\\n');\n        prompt = initial_prompt;\n        const reminder = '';\n        const context = history;\n        const entry = message;\n        const top_p = 0.0;\n        const frequency_penalty = 1.0;\n        const presence_penalty = 0.0;\n        const n = 1;\n        const payload = {\n            user_id: 'xxxxxx',\n            session: session,\n            event: eventNumber,\n            prompt: prompt,\n            reminder: reminder,\n            context: context,\n            entry: entry,\n            temperature: temperature,\n            max_tokens: max_tokens,\n            top_p: top_p,\n            frequency_penalty: frequency_penalty,\n            presence_penalty: presence_penalty,\n            n: n,\n            user_name: user_name,\n            agent_name: agent_name,\n            max_tries: 3,\n        };\n        console.log(payload);\n        const requestOptions = {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify(payload),\n        };\n        try {\n            return await fetch(API_URL, requestOptions)\n                .then((response) => response.json())\n                .then((response) => {\n                console.log(response.reply);\n                if (model == 'llama-2') {\n                    response.reply = response.reply.split('undefined')[0];\n                }\n                return response;\n            });\n        }\n        catch (error) {\n            return {\n                reply: 'Error reaching: ' + API_URL + ' Details: ' + error.message,\n                failed: true,\n            };\n        }\n    }\n}\nexport default APIPlugin;\n"]}